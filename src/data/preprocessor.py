"""
–ú–æ–¥—É–ª—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ MBTI

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ 
–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ –ª–∏—á–Ω–æ—Å—Ç–∏ MBTI –∏–∑ –ø–æ—Å—Ç–æ–≤ —Å–æ—Ü—Å–µ—Ç–µ–π.
"""

import re
import string
from typing import List, Optional, Union
import unicodedata

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import spacy


class TextPreprocessor:
    """
    –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ MBTI.
    
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
    - –£–¥–∞–ª–µ–Ω–∏–µ URL
    - –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
    - –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    """
    
    def __init__(
        self,
        lowercase: bool = True,
        remove_urls: bool = True,
        remove_special_chars: bool = True,
        remove_numbers: bool = False,
        remove_stopwords: bool = True,
        lemmatize: bool = True,
        language: str = "english",
        use_spacy: bool = False,
        spacy_model: str = "en_core_web_sm",
        min_token_length: int = 2,
        max_token_length: int = 50,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            lowercase: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
            remove_urls: –£–¥–∞–ª—è—Ç—å URL –∏–∑ —Ç–µ–∫—Å—Ç–∞
            remove_special_chars: –£–¥–∞–ª—è—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            remove_numbers: –£–¥–∞–ª—è—Ç—å —á–∏—Å–ª–∞
            remove_stopwords: –£–¥–∞–ª—è—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            lemmatize: –ü—Ä–∏–º–µ–Ω—è—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é
            language: –Ø–∑—ã–∫ –¥–ª—è —Å—Ç–æ–ø-—Å–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 'english')
            use_spacy: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å spaCy –≤–º–µ—Å—Ç–æ NLTK (—Ç–æ—á–Ω–µ–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
            spacy_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ spaCy
            min_token_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            max_token_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        self.lowercase = lowercase
        self.remove_urls = remove_urls
        self.remove_special_chars = remove_special_chars
        self.remove_numbers = remove_numbers
        self.remove_stopwords_flag = remove_stopwords
        self.lemmatize_flag = lemmatize
        self.language = language
        self.use_spacy = use_spacy
        self.min_token_length = min_token_length
        self.max_token_length = max_token_length
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ—Å—É—Ä—Å—ã NLTK
        if not use_spacy:
            self._download_nltk_resources()
            self.stopwords = set(stopwords.words(language))
            self.lemmatizer = WordNetLemmatizer()
        else:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º spaCy
            try:
                self.nlp = spacy.load(spacy_model)
            except OSError:
                print(f"–ú–æ–¥–µ–ª—å spaCy '{spacy_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ó–∞–≥—Ä—É–∂–∞—é...")
                import subprocess
                subprocess.run(["python", "-m", "spacy", "download", spacy_model])
                self.nlp = spacy.load(spacy_model)
            
            self.stopwords = self.nlp.Defaults.stop_words
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        self.url_pattern = re.compile(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        )
        self.mention_pattern = re.compile(r'@\w+')
        self.hashtag_pattern = re.compile(r'#\w+')
        self.number_pattern = re.compile(r'\d+')
        
    def _download_nltk_resources(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–±—É–µ–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."""
        resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4']
        for resource in resources:
            try:
                nltk.data.find(f'tokenizers/{resource}')
            except LookupError:
                try:
                    nltk.download(resource, quiet=True)
                except:
                    pass
    
    def clean_text(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç URL, —É–ø–æ–º–∏–Ω–∞–Ω–∏–π, —Ö—ç—à—Ç–µ–≥–æ–≤ –∏ —Ç.–¥.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not isinstance(text, str):
            return ""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º unicode —Å–∏–º–≤–æ–ª—ã
        text = unicodedata.normalize('NFKD', text)
        
        # –£–¥–∞–ª—è–µ–º URL
        if self.remove_urls:
            text = self.url_pattern.sub('', text)
        
        # –£–¥–∞–ª—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ —Ö—ç—à—Ç–µ–≥–∏ (–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ #)
        text = self.mention_pattern.sub('', text)
        text = self.hashtag_pattern.sub(lambda m: m.group(0)[1:], text)  # –£–¥–∞–ª—è–µ–º # –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = ' '.join(text.split())
        
        return text
    
    def remove_special_characters(self, text: str) -> str:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –¢–µ–∫—Å—Ç –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        """
        if self.remove_numbers:
            text = self.number_pattern.sub('', text)
        
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        text = re.sub(r"[^\w\s']", ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = ' '.join(text.split())
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        """
        if self.use_spacy:
            doc = self.nlp(text)
            tokens = [token.text for token in doc]
        else:
            tokens = word_tokenize(text)
        
        return tokens
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        """
        return [token for token in tokens if token.lower() not in self.stopwords]
    
    def lemmatize(self, tokens: List[str]) -> List[str]:
        """
        –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        """
        if self.use_spacy:
            text = ' '.join(tokens)
            doc = self.nlp(text)
            return [token.lemma_ for token in doc]
        else:
            return [self.lemmatizer.lemmatize(token) for token in tokens]
    
    def filter_tokens(self, tokens: List[str]) -> List[str]:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –¥–ª–∏–Ω–µ –∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        """
        filtered = []
        for token in tokens:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
            if len(token) < self.min_token_length or len(token) > self.max_token_length:
                continue
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            if token.isalpha():
                filtered.append(token)
        
        return filtered
    
    def preprocess(self, text: str) -> List[str]:
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        text = self.clean_text(text)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        if self.lowercase:
            text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        if self.remove_special_chars:
            text = self.remove_special_characters(text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        tokens = self.tokenize(text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
        tokens = self.filter_tokens(tokens)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        if self.remove_stopwords_flag:
            tokens = self.remove_stopwords(tokens)
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º
        if self.lemmatize_flag:
            tokens = self.lemmatize(tokens)
        
        return tokens
    
    def preprocess_batch(self, texts: List[str]) -> List[List[str]]:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            texts: –°–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        return [self.preprocess(text) for text in texts]
    
    def preprocess_to_string(self, text: str) -> str:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —Å—Ç—Ä–æ–∫–∏ –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
        """
        tokens = self.preprocess(text)
        return ' '.join(tokens)


class MBTIPostPreprocessor(TextPreprocessor):
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ø–æ—Å—Ç–æ–≤ MBTI –∏–∑ —Å–æ—Ü—Å–µ—Ç–µ–π.
    
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç MBTI-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    
    def __init__(self, *args, **kwargs):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MBTI –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–ª—è –ø–æ—Å—Ç–æ–≤."""
        super().__init__(*args, **kwargs)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω —Ç–∏–ø–æ–≤ MBTI
        self.mbti_pattern = re.compile(r'\b[IE][NS][TF][JP]\b', re.IGNORECASE)
    
    def clean_mbti_post(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ MBTI –ø–æ—Å—Ç–∞ –æ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ç–∏–ø–æ–≤ MBTI.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –£–¥–∞–ª—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–∏–ø–æ–≤ MBTI —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        text = self.mbti_pattern.sub('', text)
        return self.clean_text(text)
    
    def preprocess_posts(self, posts: Union[str, List[str]], separator: str = "|||") -> str:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å—Ç–æ–≤ –æ—Ç –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        MBTI –¥–∞—Ç–∞—Å–µ—Ç —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö '|||'.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            posts: –û–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤
            separator: –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤ —Å—Ç—Ä–æ–∫–µ –ø–æ—Å—Ç–æ–≤
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –†–∞–∑–¥–µ–ª—è–µ–º –µ—Å–ª–∏ —ç—Ç–æ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞
        if isinstance(posts, str):
            post_list = posts.split(separator)
        else:
            post_list = posts
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç
        all_tokens = []
        for post in post_list:
            post = self.clean_mbti_post(post)
            tokens = self.preprocess(post)
            all_tokens.extend(tokens)
        
        return ' '.join(all_tokens)


def create_preprocessor(
    preset: str = "default",
    **kwargs
) -> TextPreprocessor:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ —Å –ø—Ä–µ—Å–µ—Ç–∞–º–∏.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        preset: –ü—Ä–µ—Å–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ('default', 'minimal', 'aggressive', 'mbti')
        **kwargs: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–µ—Å–µ—Ç–∞
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        –≠–∫–∑–µ–º–ø–ª—è—Ä TextPreprocessor
    """
    presets = {
        "default": {
            "lowercase": True,
            "remove_urls": True,
            "remove_special_chars": True,
            "remove_numbers": False,
            "remove_stopwords": True,
            "lemmatize": True,
            "min_token_length": 2,
        },
        "minimal": {
            "lowercase": True,
            "remove_urls": True,
            "remove_special_chars": False,
            "remove_numbers": False,
            "remove_stopwords": False,
            "lemmatize": False,
            "min_token_length": 1,
        },
        "aggressive": {
            "lowercase": True,
            "remove_urls": True,
            "remove_special_chars": True,
            "remove_numbers": True,
            "remove_stopwords": True,
            "lemmatize": True,
            "min_token_length": 3,
        },
        "mbti": {
            "lowercase": True,
            "remove_urls": True,
            "remove_special_chars": True,
            "remove_numbers": False,
            "remove_stopwords": True,
            "lemmatize": True,
            "min_token_length": 2,
        }
    }
    
    if preset not in presets:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–µ—Å–µ—Ç: {preset}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(presets.keys())}")
    
    config = presets[preset]
    config.update(kwargs)
    
    if preset == "mbti":
        return MBTIPostPreprocessor(**config)
    else:
        return TextPreprocessor(**config)


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...")
    
    # –ü—Ä–∏–º–µ—Ä MBTI –ø–æ—Å—Ç–∞
    sample_text = """
    Hey everyone! Check out this cool article: https://example.com/article
    I'm an INTJ and I really love programming #coding @friend Let's connect!!!
    123 people liked my post... üöÄ
    """
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    preprocessor = create_preprocessor("mbti")
    
    print(f"\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{sample_text}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    tokens = preprocessor.preprocess(sample_text)
    print(f"\n–¢–æ–∫–µ–Ω—ã: {tokens}")
    
    processed_text = preprocessor.preprocess_to_string(sample_text)
    print(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {processed_text}")
    
    # –¢–µ—Å—Ç MBTI-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    mbti_preprocessor = MBTIPostPreprocessor()
    multi_post = "I love coding!|||Check out my project|||INTJ here"
    processed_posts = mbti_preprocessor.preprocess_posts(multi_post)
    print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –º—É–ª—å—Ç–∏-–ø–æ—Å—Ç–æ–≤: {processed_posts}")
    
    print("\n‚úÖ –¢–µ—Å—Ç –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –ø—Ä–æ–π–¥–µ–Ω!")

